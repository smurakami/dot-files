\section{関連研究}
\label{sec:related}
本研究の提案手法には二人称視点映像の利用，映像中での動作認識，視線情報の利用という要素がある．
本章ではそれぞれの要素に関連する先行研究を紹介する．
まず最初に\ref{sec:related/second_person}節で二人称視点映像の利用に関する先行研究研究について紹介する．
次に，\ref{sec:related/action_recog}節で映像中での動作認識に関する先行研究について紹介する．
最後に\ref{sec:related/gaze}視線情報の利用に関する先行研究について紹介する．

\subsection{二人称視点映像の利用}
\label{sec:related/second_person}

本節では二人称視点映像を利用した先行研究について紹介する．

ある人物Aがウェアラブルカメラを用いて一人称視点映像を記録しつつ，別の人物Bとやり取りを行う状況を考える．
このとき，人物Bが映り込んだ一人称視点映像を，\emph{Bに対する二人称視点映像}と定義する．

このような二人称視点映像では，従来利用されてきた固定カメラによる三人称視点映像と比較して人物の詳細な映像を得ることができる．
こうした特性から，二人称視点映像は人物同士のやり取りや関連性を観測するための手段として活用されてきた．

二人称視点映像を利用した取り組みとして，Fathiらの研究\cite{FathiHR12}とAlletltoらの研究\cite{alletto2014_383}が挙げられる．
Fathiらは，被験者の集団がテーマパークを観光する様子を収録した1日分の二人称視点映像を分析することで，人物間の相互作用の検出を行った．
被験者の集団は30人ほどの規模で，そのうち8人がヘッドマウントカメラを装着した．
この研究では，映像中に映り込んだ人物の頭部位置と角度を推定することで人物間の顔向けや，複数の人物が共通の場所に注目する状態を検出した(図\ref{fig:related_fathi_1})．
また，顔向けが発生した時点で人物間でやり取りが生じていると推定することで人物間のやり取りを検出した．
加えて，カメラ装着者の視界への人物の映り込みを集計することでどの人物とどの人物が親しい関係にあるか，といった人物関係を推定した．

\begin{figure}[t]
\begin{center}
    \includegraphics[width=\linewidth]{images/related_fathi_1.ai}
\end{center}
\caption[二人称視点映像における人物の顔向け解析の例]{
二人称視点映像における人物の顔向け解析の例(\cite{FathiHR12}より)．
Fathiらは二人称視点映像中に現れる人物の頭部位置と向きを推定することで，人物間の相互作用の検出を行った．
人物同士の顔向けは映像中の直線で示されている．
この顔向けを検出することで人物間のやり取りの発生を推定した．
また，映像中で共通の部分に注目している人物は同じ色で表されている．
}
\label{fig:related_fathi_1}
\end{figure}

一方Allettoらの研究\cite{alletto2014_383}では二人称視点映像中に現れる人物の頭部位置と向きから人物間の関係を推定し，
クラスタリングの手法を用いてどの人物とどの人物が同じグループに属するかというグループ分けを推定した(図\ref{fig:related_alletto}).

\begin{figure}
\begin{center}
    \includegraphics[width=12cm]{images/related_alletto.ai}
\end{center}
\caption[二人称視点映像における人物の顔向け解析の例]{
二人称視点映像における人物の顔向け解析の例(\cite{alletto2014_383}より)．
Allettoらは二人称視点映像中での人物の頭部位置と向きを3次元空間上で推定し，その情報からクラスタリングにより人物のグループ分けを推定した．
人物の頭部の色は，それぞれの人物がどのグループに所属するかを表している．また，赤色の丸はカメラを装着した人物を示している．
}
\label{fig:related_alletto}
\end{figure}

これらの研究では二人称視点映像に現れる人物の頭部の位置と向きを手掛かりに人物同士の関係性を推定することを可能にしている．
一方で，やり取りの内容を含めたより詳細な分析のためには，映像中で起きている動作種類の推定まで行えることが望ましい．

二人称視点映像中での動作種類を識別した研究に\cite{RyooM13}が挙げられる．
本手法ではヘッドマウントカメラを人物に見立てた人形に固定し，``握手をする"や``手を振る"といった動作を識別した．
これらの動作はいずれも映像記録者と他者の間のやり取りを想定したものであった．

Ryooらの研究では二人称視点映像は人形に固定されたカメラから収録されたものであり，自発的な頭部運動は含まれない．
一方，実際の二人称視点からの映像は頻繁に頭部運動の影響を受け頭部運動から生まれる動作特徴が識別精度に影響をもたらす可能性がある．
そこで本研究では実際に人物の頭部に取り付けられたカメラによる，自発的な頭部運動が含まれる二人称視点映像における動作認識ついて検討する．

\begin{figure}
\begin{center}
    \includegraphics[width=6cm]{images/related_ryoo_1.ai}
\end{center}

\caption[Ryooらによる二人称視点映像動作認識のための実験装置]{
Ryooらによる二人称視点映像動作認識のための実験装置(\cite{RyooM13}より)．
Ryooらの研究では，このような人形にウェアラブルカメラを装着することで二人称視点映像からの動作を収録した．
}
\label{fig:related_alletto}
\end{figure}

\begin{figure}
\begin{center}
    \includegraphics[width=\linewidth]{images/related_ryoo_2.ai}
\end{center}
\caption[Ryooらによる二人称視点映像動作認識における動作種類の例]{
Ryooらによる二人称視点映像動作認識における動作種類の例(\cite{RyooM13}より)．
Ryooらの研究では``握手"，``手を振る"などの7種類の動作を選定し，動作認識を行った．
これらの動作はいずれも，カメラ装着者と相手の間のやり取りによって生じるものである．
}
\label{fig:related_alletto}
\end{figure}


\subsection{映像中での動作認識}
\label{sec:related/action_recog}

映像中での人物動作の特徴表現としては，映像中の身体の部位を認識した上でその位置と動きから特徴を抽出する部位ベースの手法\cite{Jammalamadaka15}と，
画面全体から局所特徴を抽出した上でFisher Vector(FV)\cite{fisher_vector}などのコーディング手法を用いて高次特徴を生成する局所特徴ベースの手法\cite{wang2011, IDT, TDD}が挙げられる．
この中で部位ベースの手法は部位の認識に失敗した場合にそれ以降の処理が全て失敗してしまうため，観察対象の対象の激しい動きや画面外への離脱，視点移動などの影響を受けやすい．
こうした性質から部位ベースの手法は，常に装着者の頭部運動の影響を受けるヘッドマウントカメラ等の映像を扱う際に必ずしも有効に働くとは限らない．
そこで本研究では局所特徴ベースの手法を採用し動作認識に使用した．

特徴ベースの動作認識手法としてこれまでにon space-time interest points\cite{STIP}，dense trajectories(DT)\cite{wang2011}, improved dense trajectories(IDT)\cite{IDT}などが提案されてきた．
この中でもIDTはカメラ運動がある場合にも利用可能な手法で，カメラ運動を推定しながら特徴抽出が行われる．
IDTではoptical flow\cite{Horn81determiningoptical}を用いて映像中の特徴点を追跡し，その周辺のhistograms of oriented gradients(HoG)\cite{HOG}, histgrams of oriented optical flow(HOOF)\cite{HOGHOF}, motion boundary histogram(MBH)\cite{MBH}などの局所特徴量を抽出する．
この中でHOOFやMBHといったoptical flowから算出される特徴量を計算する際に，カメラ運動に起因するoptical flowのバイアスを推定しキャンセルしてから特徴量を算出する．
このことによりヘッドマウントカメラ等でも頑健に特徴中抽出を行うことができる．

\begin{figure}
\begin{center}
    \includegraphics[width=\linewidth]{images/related_idt.ai}
\end{center}

\caption[Improved dense trajectoriesの概要]{
Improved dense trajectories(IDT)の概要(\cite{IDT}より)．
IDTでは，局所特徴抽出の際にカメラ運動の影響を差し引いたoptical flowを用いることにより，カメラ運動がある映像でも頑健な特徴抽出を可能にした．
図の1行目が入力映像，2行目が入力映像から抽出されたoptical flow，3行目がカメラ運動の影響を差し引いたoptical flow，4行目が最終的に生成されたIDTの軌跡である．
4行目ではIDTの特徴点の座標を赤点，特徴点の軌跡を緑線，カメラ運動の推定により取り除かれた特徴点の軌跡を白線で表している．
カメラ運動による背景部分のIDTが取り除かれていることが確認できる．
} \label{fig:related_alletto}
\end{figure}


IDTでは，HoG, HOOF, MBHといった手法を用いて局所特徴の抽出を行った．
一方，静止画像における一般物体認識ではこのような人手により設計された特徴表現に代替する手法としてconvolutional neural network(CNN)\cite{CNN}が注目されている．
CNNでは識別に用いる多層ニューラルネットのそれぞれの層が特徴抽出器の役割をしているため，特徴抽出の設計自体を学習することができる．

このようなCNNの特性を動作認識に利用する手法としてtwo-stream convolutional neural networks(TCNN)\cite{TCNN}が挙げられる．
TCNNでは，映像と映像から抽出されたoptical flowの2つを入力として用いる．
この2つの入力を2つのCNNでそれぞれ独立して処理し，最後に結果を統合して用いることでCNNを用いて動作認識を行うことを可能にした．


\begin{figure}
\begin{center}
    \includegraphics[width=\linewidth]{images/related_tcnn.ai}
\end{center}

\caption[two-stream convolutional neural networksの概要]{
two-stream convolutional neural networks(TCNN)の概要(\cite{TCNN}より)．
TCNNでは，spatial stream ConvNetとtemporal stream ConvNetの2つの多層ニューラルネットにより，
入力の映像とそのoptical flowそれぞれ別に処理する．
最終的に2つの多層ニューラrネットの結果を統合することにより，動作種類の識別等の結果を得る．
} \label{fig:related_alletto}
\end{figure}


TCNNの特徴表現を学習する能力とIDTのカメラ運動に対する頑健性を組み合わせた手法としてtrajectory-pooled deep-convolutional descriptors(TDD)\cite{TDD}が提案された．
TDDではIDTで使用するHoG, HOOF, MBHの代わりにTCNNで抽出された特徴マップを利用することで，従来のIDTと比較してより高精度の動作認識が可能となっている．
本研究ではこのTDDを用いて局所特徴を生成する．

このようにして生成された局所特徴量群から高次特徴量を生成した上で識別に用いる．
\cite{wang2011}, \cite{IDT}では映像全体の局所特徴からbag of features\cite{BOF_1, BOF_2}やFisher vector\cite{fisher_vector}を用いて高次特徴量を生成した．
一方，背景の影響や他の人物の映り込みの影響に対する頑健性のためには，映像全体の局所特徴を用いるのではなく有用な特徴量を選択しつつ利用する手法が望ましい．

\cite{Ni_2015_CVPR}では局所特徴を座標と時間位置を元にクラスタリングで複数の部位に分け，生成されたそれぞれの部位について重要度を推定した．
この重要度を元にそれぞれの局所特徴に重み付けを行い動作認識を行った．
これに対して提案手法では映像記録者が会話の中でどこに注目を向けているかという視線情報を得ることができる．
そこで，視線情報を用いて局所特徴の重要度を推定し動作認識の精度を向上できるか検討する．

\subsection{視線情報の利用}
\label{sec:related/gaze}


近年，視線計測装置の小型化，低コスト化に伴い視線情報の研究利用が盛んに行われるようになった．
これらの研究により，様々な識別タスクにおける視線情報の有用性が示されてきた．

Yunら\cite{kiwon_cvpr13_gaze}は静止画像中での一般物体認識に視線の情報を用いる手法を提案した．
Yunらは被験者が静止画像を見る際の視線の動きを計測した．
また同時に，画像中に物体がどのように写り込んでいるかについて自然言語により説明された文章を被験者から集めた．
このようにして集められた視線情報，言語情報と画像情報を統合することで静止画像に写り込んでいる物体の種類と位置を識別した．

視線情報を利用した動作認識に関する研究としてはFathiらの\cite{Fathi:2012}やShapovalovaらの手法\cite{shapovalova}が挙げられる．
Fathiらはヘッドマウントカメラによる一人称視点映像と，ヘッドマウントカメラに設置された視線計測機器による視線情報を利用することで``料理"や``歯磨き"といった手もとを見ながら行う日常動作を学習した．
Fathiらは一人称視点映像中で視線が止まる注視点を検出し，その注視点周辺の画像特徴を抽出することで動作特徴を生成した．

一方，Shapovalovaら\cite{shapovalova}は他人の動作を観察する三人称視点映像中での動作認識手法を提案した．
ここで言う三人称視点映像とは，スポーツ競技映像など固定カメラから撮影された映像のことである．
Shapovalovaらは三人称視点映像中で動作認識の際に，映像を被験者に見てもらい，映像閲覧者の画面上での視線の位置を計測することで映像中の動作の位置情報を推測しつつ動作を識別した．

% FathiらとShapovalovaらは三人称視点からの視線情報を利用した認識手法を提案したが，本研究で扱う二人称視点映像は以下の点で三人称視点映像とは異なる性質を持っている．
このように一人称視点，三人称視点からの視線情報を利用した認識手法が研究されてきた．
これに対して本研究では，二人称視点での視線を利用した認識手法について検証する．

\newpage

