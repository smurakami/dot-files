\section{データセット}
\label{sec:dataset}

\subsection{概要}

本研究では，二人称視点映像における映像の記録者の視線位置を利用することにより動作認識の精度を上げる手法を提案する．
映像の記録者の視線位置を含む二人称視点映像は存在しないため，今回は提案手法の評価のために新たにデータセットを構築した．
データセット収録の際には，被験者が視線計測機器と一体になったウェアブルカメラを装着し，他の被験者とやり取りをするという設定でデータ収集を行った．

本データセットでは人物同士のやり取りで発生すると考えられる12の動作種類について，その二人称視点からの動作映像の収録を行った．
収録の際には6人の被験者が動作認識の対象として参加した．
また，2人の被験者が二人称視点映像の記録者として実験に参加した．
最終的に各動作種類で108サンプル，全体で1296サンプルの動作映像を収録した．
収集したデータセットの各動作種類の映像例を図\ref{fig:dataset_samples_0}に示す．

\begin{figure}
\begin{center}
    \includegraphics[width=12cm]{images/dataset_samples_0.ai}
    \caption[データセットの映像例(1/2)]{データセットの映像例(1/2)．黄丸は視線位置を示す．}
    \label{fig:dataset_samples_0}
\end{center}
\end{figure}

\begin{figure}
\begin{center}
    \includegraphics[width=12cm]{images/dataset_samples_1.ai}
    \caption[データセットの映像例(2/2)]{データセットの映像例(2/2)．黄丸は視線位置を示す．}
    \label{fig:dataset_samples_1}
\end{center}
\end{figure}

\subsection{動作クラスの選定}

本研究で作成したデータセットでは，人物間のやり取りで発生する動作を12種類選定し収録した．
人物間のやり取りで発生する動作には，{ \bf 指差し }や{ \bf 挙手 }などの手振りによる動作と，{ \bf うなずき }や { \bf 顔向け }といった頭の動きによる動作が考えられる．
そこで，収録する動作種類には，手による動作と頭部運動による動作が含まれるように選定を行った．
本データセットで収録した動作クラスの一覧を表\ref{tb:action_class}に示す．

\begin{table}[tb]
\begin{center}
    \caption{本研究で収録した動作映像の種類}
    \label{tb:action_class}
        \begin{tabular}{|c|c|}
            \hline
            身体部位         & 動作種類 \\ \hline \hline
                             & 指差す \\ \cline{2-2}
                             & 呼びかける \\ \cline{2-2}
                             & 主張する \\ \cline{2-2}
            手による動作     & 手を挙げる \\ \cline{2-2}
                             & 腕組み \\ \cline{2-2}
                             & 手を頭に乗せる \\ \cline{2-2}
                             & 物を拾う \\ \hline

            手と頭による動作 & 考える \\ \hline

                             & 頷く \\ \cline{2-2}
            頭による動作     & 首を傾げる \\ \cline{2-2}
                             & こちらを向く \\ \cline{2-2}
                             & 顔を上げる \\ \hline
        \end{tabular}
    \end{center}
\end{table}

\subsection{データセットの収集}

本データセットでは，6人の被験者の動作映像を屋内，屋外を含む3ヶ所で収録した．
二人称視点映像と視線情報は2人の被験者から収集し，収録にはPupil Labs社のPupil Pro\footnote{http://pupil-labs.com}を使用した．
% データセットの収録風景の一例を図\ref{fig:recording_dataset}に示す．
データセットの収録の様子を図\ref{fig:dataset_recording}に示す．

\begin{figure}
\begin{center}
    \includegraphics[width=\linewidth]{images/recording_dataset.ai}
\end{center}
\caption[データセットの収録風景の一例]{
    データセットの収録風景の一例．
    1人の二人称視点映像記録者と2人の動作認識対象の被験者が互いに向かい合い座った状態でデータセットの収録を行った．
    映像の記録者の視界には常に2人の解析対象者が映り込むようにして収録を行った． }
\label{fig:dataset_recording}
\end{figure}

データセットの収録の際には，映像の記録者(observer)と被験者2人が向かい合い座った状態でデータセットの収録を行った．
このようにすることで映像の記録者の視界に同時に2人の映像が映るようになる．
これは視線を利用することで複数人物が同時に別の動作をする，といった場合に認識精度を向上することが可能であるか評価することを目的としている．

収録された動作群は映像の記録者と動作認識の対象者との間のコミュニケーションとして行われるようにしており，動作認識の対象者同士のコミュニケーションとした行われた動作は含まれないようにしている．
よって映像の記録者の視線は動作認識の対象者との間のコミュニケーションの一部としての振る舞いをする．

データセットの収録の際にはあらかじめ作成した台本を読み上げ，それに従って被験者が動作することで一連の動作群を収録した長い動画を収録した．
収録された長い動画の中から，動作が含まれる部分を切り出すことで動作サンプル群を作成した．


\subsection{動作サンプルのフォーマット}

本データセットにおける二人称視点映像は30fpsのフレームレート，解像度320px $\times$ 180pxのフォーマットで収録した．
解像度は，一度収録した高解像度の映像の解像度を落とすことにより320px $\times$ 180pxに変更した．
これは，dense trajectoriesの生成やCNNによる特徴マップの生成を実用的な計算資源で実現するためである．

それぞれの映像サンプルの中には一つの動作が収録されており，映像の長さは動作の起点から動作の終了までの長さと大まかに等しくなるように編集されている．
ここでは，例えば``挙手"の動作であれば手を上げる動きが始まる瞬間を動作の起点と定義する．
今回収録した動作はほとんどが1.5秒で完結する動作であったため，映像サンプルの長さを1.5秒に統一した．

\subsection{動作サンプルに対するアノテーション}

動作認識の教師データとするため，それぞれの動作サンプルに対してその動作種類を付与した．
また，教師データとテストデータの間で認識対象の人物を分離する交差検定を可能にするため，各動作サンプルに対してその動作の主体が誰であるかといった人物ラベルを付与した．
本研究の実験では比較手法として，人物領域の局所特徴のみを使用する手法を評価する．
そのために，動作をしている人物を含む矩形領域を人手で付与した．
矩形領域の際には，動作の起点の時点での人物領域を矩形で付与し，その後の人物位置は矩形領域内で生成したdense trajectoriesの重心を矩形領域で追跡することで各フレームでの人物領域の情報を付与した．
付与された人物領域の例を図\ref{fig:bounding_box}に示す．

\begin{figure}
\begin{center}
    \includegraphics[width=\linewidth]{images/bounding_box.ai}
    \caption[付与された人物領域の例]{
        付与された人物領域の例．
        赤色の矩形が付与された矩形領域である．
        まず$t=0$の時点での人物領域を人手で付与し，それ以降の人物領域はdense trajectoriesの重心を追跡することで判定している．
        }
    \label{fig:bounding_box}
\end{center}
\end{figure}

\newpage
