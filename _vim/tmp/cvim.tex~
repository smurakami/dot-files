%%
%% 研究報告用スイッチ
%% [techrep]
%%
%% 欧文表記無しのスイッチ(etitle,jkeyword,eabstract,ekeywordは任意)
%% [noauthor]
%%

\documentclass[submit,techrep]{ipsj}
%\documentclass[submit,techrep,noauthor]{ipsj}



\usepackage[dvips]{graphicx}
\usepackage{latexsym}
\usepackage{bm}

\def\Underline{\setbox0\hbox\bgroup\let\\\endUnderline}
\def\endUnderline{\vphantom{y}\egroup\smash{\underline{\box0}}\\}
\def\|{\verb|}
\def\newblock{\hskip .11em plus .33em minus .07em}

\setcounter{巻数}{53}%vol53=2012
\setcounter{号数}{10}
\setcounter{page}{1}


\begin{document}


\title{視線を利用した二人称視点動作認識}

\etitle{Second person action recognition using gaze}

\affiliate{seiken}{東京大学　生産技術研究所, 〒 153-8505 東京都目黒区駒場 4-6-1}

\author{村上 晋太郎}{Shintaro Murakami}{seiken}[shin-m@ut-vision.org]
\author{米谷 竜}{Ryo Yonetani}{seiken}[yonetani@iis.u-tokyo.ac.jp]
\author{佐藤 洋一}{Yoichi Sato}{seiken}[ysato@iis.u-tokyo.ac.jp]

\begin{abstract}
　近年，ウェアラブルカメラを用いた一人称視点映像が盛んに利用されている．
一人称視点映像中では，映像の記録者の視界に別の人物が映り込むことにより，映り込んだ人物にとっての二人称視点映像が生じる．
二人称視点映像では，定点カメラなどによる三人称視点映像と比べて，対象の手元の動きなどの細かい情報を捉えることができる．
本研究ではこの二人称視点映像中での人物の動作認識について検討する．
　
　従来の動作に認識では，画面全体の動きから特徴量を生成する手法が主流であった．
しかしながら，二人称視点映像中では映像の背景が動いたり映像中に第三者が映り込むことで，対象人物とは関係のない動きの情報が特徴量に含まれてしまう．
そこで，本研究ではウェアラブルカメラと一体となった視線計測装置を用い，映像の記録者が注視している位置との関係から画面中のそれぞれの部位の重要度を考慮しつつ特徴量を生成することで，認識精度の向上を目指す．　
\end{abstract}

\maketitle


%         _____
%        / __  \
%       |\/_|\  \
%       \|/ \ \  \
%            \ \  \
%             \ \__\
%              \|__|


\section{はじめに}

近年，カメラ機能のついた様々なウェアラブルデバイスの普及に伴い，一人称視点映像の利用が盛んに行われるようになった．
一人称視点映像中では，映像の記録者の視界に別の人物が映り込むことにより，映り込んだ人物の二人称視点からの観察が記録される．
こうした二人称視点映像中での


これらの映像は装着者の視点を追跡し続けるため，俯瞰カメラや監視カメラなどの固定された視点による三人称視点映像と比較して，装着者の注目している事象を詳細に観測することができる．

(研究背景の説明)

\section{関連研究}
\label{related}
\subsection{二人称視点映像の利用}

二人称視点から人物映像解析に関するする研究として，Stefanoらの研究\cite{alletto2014_383}やAlirezaらの研究\cite{FathiHR12}が挙げられる．
これらの研究では一人称視点映像に現れる人物の頭部の位置と向きを推定し，それを元に登場する人物の「位置関係」と人物同士の「顔向け」の発生を観測することで人物間の関係性を推測した．
しかしながら顔向けや位置関係だけでは人物間でのコミュニケーションがポジティブな意思表示なのかネガティブな意思表示なのかといった詳細な分析は難しい．
また，会議室での会議など，人同士の位置関係が固定されている環境下では同様の手法は適用できない．
そこで，どのような種類の動作が起きているのかといった動作の種類の認識が出来ることが望ましい．

(Michel ryoの研究に言及)

Michelらの研究では二人称視点からの動作解析で有効な結果を残したが，使用されたヘッドマウントカメラは人物に見立てた人形に固定されたものであった．
しかしながら実際の二人称視点からの映像は頻繁に頭部運動の影響を受け，頭部運動から生まれる動作特徴が識別精度に影響をもたらす．
そこで，頭部運動に由来するグローバルモーションにうまく対処できるような動作特徴表現が必要となる．

\subsection{動画像の特長表現}
\label{move_feat}

固定カメラ視点からの動画像の特徴表現としては，身体の部位の位置情報から特徴を抽出する手法が一般的であった．
しかしながら，このような手法は部位の認識に失敗するとそれ以降の処理が全て失敗してしまうため，観察対象の対象の激しい動きや画面外への離脱，カメラ移動などの影響を受けやすい．
そのため，常に装着者の頭部運動の影響を受ける一人称視点映像を扱うには適切でないと言える．

そこで，カメラ移動に頑健な動作特徴量としてDense trajectories\cite{wang2011}が提案された．
Dense trajectoriesは映像中の特徴点を追跡し，その周辺のHOG\cite{HOGHOF}, HOF\cite{HOF}, MBH\cite{MBH}などの局所特徴量をまとめて特徴量とし，動作認識に利用する手法である．
Dense trajectoiresは部位認識などの失敗しやすい工程を経ずに特徴抽出を行うため，ノイズの影響に頑健である．

Dense trajectoriesはカメラ運動にうまく対処できる手法であるが，利用している特徴量はHOG, HOF, MBHといった局所特徴量であった．
一方，静止画像における一般物体認識では局所特徴量に変わる手法としてCombolutional Newral Network(CNN)\cite{CNN}が注目を浴びている．
CNNの内部では複数のニューラルネットワークの層が前段から順番に入力を処理し，前段では輝度勾配などの小さな特徴を捉え後段でそれらの特徴をまとめて大きな構造の特徴とする．
そのため，画像の各領域が「どのくらい顔らしいか」「どのくらい机らしいか」といった大きな構造を特徴として捉え利用することができる．

そこで，Dense trajectoriesの頑健性とCNNの識別力を組み合わせた手法であるTrajectory-Pooled Deep-Convolutional Descriptors(TDD)が提案された．
TDDは，CNNによるDeep featureの生成とDense trajectoriesによる特徴量の収集を組み合わせた手法である．
TDDではDense trajectriesで使用するHOG, HOF, MBHの代わりにCNNで抽出されたDeep featureを利用する．

このように，カメラ移動をうまく扱う手法としてtrajectorieベースで特徴点を追跡しつつ特徴量を収集する手法が提案されてきた．
これらは動画全体から特徴量を生成した後に，Bag of feature\cite{BOF}やFisher vector\cite{fisher_vector}などコーディング手法で一つにまとめることが一般的である．
しかしながら動画全体の特徴量をまとめるだけでは，依然として激しいカメラ移動に由来する背景特徴量が混ざり込むことで認識精度に影響を与えてしまう．
また，画面内に複数の人物が映り込みそれぞれ動くような場合にも認識に失敗してしまう．
そこで二人称視点からの動作解析には，動画全体の特徴から解析対象の動作に由来する特徴を適切に選択するような手法が必要となる．

\subsection{視線情報の利用}
近年，視線計測装置の小型化，低コスト化に伴い視線の研究利用が盛んになっている．

Kiwonらは人間が静止画像を見る際の視線の動きを計測することで，画像中に写っている物体の種類を推定する手法を提案した．\cite{kiwon_cvpr13_gaze}

(説明)

Alienzaらの研究は，被験者の静止画像上の視線位置を利用したものであった．
一方，視線計測装置が小型化したことによりウェアラブルデバイス上でも視線を計測することが可能になり，従来ディスプレイ上に限られていた視線計測の応用領域が実世界上に拡大した．
実世界上の視線情報を利用した研究としてはAlienzaらの研究が挙げられる\cite{Fathi:2012}
Alirezaらはヘッドマウントカメラによる一人称視点映像中の視線位置の計測を利用することで，手もとを見ながら行う動作を学習した．
この動作には，料理や歯磨き粉を歯ブラシに乗せるなどの日常的な動作が含まれる．

Kiwonらの研究では被験者自身の動作を視線を利用して認識するものであった．
一方，被験者の視線を利用して他者の動作認識をする手法としてNataliyaらの手法\cite{shapovalova}が挙げられる．
Nataliyaらは，一般的なカメラで撮影された三人称視点映像の中での視線情報を利用することで映像中の人物の動作の弱教師学習を行った．

(説明)

Nataliyaらは視線を利用した動作認識の手法を提案したが，三人称視点映像に限定されたものであった．
本研究では二人称視点からの映像で動作認識を行うために，\ref{move_feat}節で述べたようなカメラ動作に頑健な動作特徴と組み合わせて識別を行う手法を提案する．

\section{提案手法}

\subsection{概要}

図\ref{}は，提案手法の概要を示したものである．
提案手法では短い一人称視点映像について，その動画中でどのような動作が行われているかを識別する．
それぞれの動画の中には一つの動作が収録されており，動画の長さは動作の起点から動作の終了までの長さと大まかに等しくなるように編集されている．
動画中には動作解析の対象となる人物が全フレームにわたって映り込んでいるように収録されている．
また，動画の各フレームには付加情報として，記録者の視線位置の情報が与えられている．

提案手法ではまず解析対象の動画全体から局所特長を抽出し，各部位から抽出された局所特長をFisher vectorなどのコーディング手法で一つにまとめ上げることで動画特徴を生成する．
このようにすることで，人物の頭，腕，胴体の部位の識別を用いて動画特徴を生成する手法に比べて頭部運動などによる画面のぶれに頑健な特徴生成を行うことができる．

生成された局所特徴量をまとめて動画特徴を生成する際に，それぞれの局所特徴がどれだけ重要であるかを推定し，その重要度を考慮しつつ動画特徴を生成する．
提案手法ではこの重要度の推定の際に視線位置を用いる．

最終的に生成された動画特徴に線形識別器を利用した他クラス分類手法を適用することで，それぞれの動画に含まれる動作があらかじめ定義された動作の種類のうちどの種類に該当するのかを判別する．

\subsection{視線を考慮した動画特徴量の生成}

一般に，人は他人の動作に注目する傾向にある．また，人は注目している視野領域に視線を移動する傾向にある．
そのために，一人称視点映像の記録者の視線の周りでは重要な動作が起こりやすいという仮定を置くことができる．
この仮定のもと，局所特徴の重要度は以下のように表わされる．

\begin{equation}
    w(\bm{x}) = f(r_x)
\end{equation}

ここで，$\bm{x}$は局所特徴ベクトル，$w(\bm{x}$は$\bm{x}$の重要度，$r_x$は$\bm{x}$の視線位置からの距離，$f$は視線位置を引数に取る関数である．

(視線が抜けている場合の対応など詳しく書く)

\subsection{実装}

本節では，提案手法のそれぞれの段階で採用した手法についてその詳細を示す．
局所特徴量の抽出には，\ref{local_feat}で説明したTDDを採用する．
(具体的なパラメータについて説明)

視線情報の収集にはPupil pro\footnote{http://pupil-labs.com}を用いる．
Pupil proはヘッドマウントカメラに視線計測装置を組み合わせることで，実世界上での視線計測を実現したデバイスである．

視線を用いた局所特徴の選択には，以下の関数を採用する

\begin{eqnarray}
f(r)=\left\{ \begin{array}{ll}
1 & (r \le r_{\theta}) \\
0 & (r > r_{\theta}) \\
\end{array}
\right.
\end{eqnarray}

% この関数により...

(以下，実装の詳細)

\section{実験}

\subsection{データセット}

本節では本研究で作成したデータセットについて述べる．
本データセットでは6名の被験者を動作認識の対象として，で12種類の動作の収録を行った．
6名の被験者から2名ずつ計3組が動作認識の対象となり，屋内と屋外を含む3種類の環境で計9パターンの設定で収録を行った．
視線位置の計測対象である動画の記録者は2名の被験者が担当した．

本データセットでは，映像の記録者を含む3人の被験者が互いに向き合い会議をしているという設定で収録を行った．
映像中には映像の記録者以外の2人が映り込むように設定した．
動作の種類は，会議において頻繁に起きると考えられる動作を12種類選定し収録した．
本データセットで選定した12種類の動作のリストを表\ref{}に示す．

会議を記録した長い動画の中から動作の起きている部分を抜き出す形で，それぞれの動作で108サンプルずつ，全ての動作で1296サンプルの短い動作映像を生成した．
今回収録したほとんどの動作がほぼ1.5秒で完結する動作であることから，それぞれの動作映像は動作の起点を開始として1.5秒の長さで生成した．
ここでは，例えば「挙手」の動作であれば手を挙げる動きが始まる瞬間を動作の起点と定義する．
動作映像の各フレームには，映像の記録者の視線位置の情報を付与した．
またベースラインの作成のため，それぞれの動作映像には動作の開始時点での身体位置を長方形の領域で付与した．

\subsection{ベースライン}

本実験では，ベースラインとして画面中の全てのtrajectoriesからFisher vectorを生成する{\bf ALL TRAJ}，
画面中の人物領域を人手で指定してその部分のみのtrajectoriesからFisher vectorを生成する{ \bf BOUNDING BOX }の二つの手法を評価した．

{\bf ALL TRAJ}は従来手法に対応し，カメラモーションにより生成される背景のtrajectoriesや観察対象ではない人物の動きに由来するtrajectoriesもFisher vectorに含まれるため，精度が低下する．
それに対して{ \bf BOUNDING BOX }は理想的なtrajectories選択に対応し，認識対象の動作に関係のあるtrajectoriesを選択するために{\bf ALL TRAJ}と比べて認識精度が向上する．

\subsection{提案手法}

提案手法では，視線を利用して重要度の高いtrajectoriesを選択することにより，前項の{\bf ALL TRAJ}より精度を向上させ，{ \bf BOUNDING BOX }に近い精度を達成することを目指す．

視線の半径は...

(視線の半径や，trajectorieがどのくらい視線とオーバーラップしたらFishser vectorに含むかなどのパラメータの説明をする)

\subsection{実験結果}

実験結果を表\ref{}に示す．

(結果の考察をする．特に，BOUNDING BOXよりもうまくいかなかった動作，うまくいった動作について，その原因を考察する．)

\section{おわりに}

本研究では... (まとめ)

\bibliographystyle{jplain}
\bibliography{cite}

\end{document}
